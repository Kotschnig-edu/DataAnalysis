\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{float}

\geometry{a4paper, margin=1in}

\title{Data Analysis Final Assignment Report}
\author{% 
  Team: Analog Avengers\\ 
  Eingang Fabian
  \And
  Kotschnig Thomas
  \And 
  Krenn Matthias 
  }

\date{}

\begin{document}
\maketitle

\section{Contributions}
\begin{itemize}[leftmargin=*]
    \item Eingang Fabian: Dataset selection and acquisition, Data quality analysis and preprocessing pipeline
    \item Kotschnig Thomas: Visualizations and EDA, Probability analysis tasks
    \item Krenn Matthias: Regression modeling and interpretation, Report writing and figure polishing

\end{itemize}

\section{Dataset Description}
\begin{itemize}[leftmargin=*]
    \item "Bike sales in Europe" from https://www.kaggle.com/datasets/sadiqshah/bike-sales-in-europe
    \item It has more than 100k entries of sales data from different countries. Streching from 2011 to 2016, with a daily sampling frequency.
    \item Key variables analyzed: customer age, order quantity, unit cost, unit price, profit, cost, revenue
    \item Shape: 113036 rows x 18 columns
    \item No missing data, however, the entry of some dates is missing completely. 
    This resolves in no missing data, but inconsistent time series. 
    There is only one bigger gap, therefore we decided for it to be okay.
\end{itemize}

\section{Task 1. Data Preprocessing and Basic Analysis}

\subsection{Basic statistical analysis using pandas}
Statistical summary of key numeric variables was obtained using pandas \texttt{describe()} function:
\begin{table}[h!]
\caption{Desriptive statistics}
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{|l|r|r|r|r|r|r|r|}
\hline
 & \textbf{Customer Age} & \textbf{Order Qty} & \textbf{Unit Cost} & \textbf{Unit Price} & \textbf{Profit} & \textbf{Cost} & \textbf{Revenue} \\
\hline
Count & 113036 & 113036 & 113036 & 113036 & 113036 & 113036 & 113036 \\
Mean  & 35.92  & 11.90  & 267.30 & 452.94 & 285.05 & 469.32 & 754.37 \\
Std   & 11.02  & 9.56   & 549.84 & 922.07 & 453.89 & 884.87 & 1309.09 \\
Min   & 17     & 1      & 1      & 2      & -30    & 1      & 2 \\
25\%  & 28     & 2      & 2      & 5      & 29     & 28     & 63 \\
50\%  & 35     & 10     & 9      & 24     & 101    & 108    & 223 \\
75\%  & 43     & 20     & 42     & 70     & 358    & 432    & 800 \\
Max   & 87     & 32     & 2171   & 3578   & 15096  & 42978  & 58074 \\
\hline
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Grouped summary of Revenue, Profit, and Order Quantity by Country}
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{|l|ccc|cc|cc|}
\hline
\textbf{Country} &
\multicolumn{3}{c|}{\textbf{Revenue}} &
\multicolumn{2}{c|}{\textbf{Profit}} &
\multicolumn{2}{c|}{\textbf{Order Quantity}} \\
\hline
 &
\textbf{Sum} & \textbf{Mean} & \textbf{Count} &
\textbf{Sum} & \textbf{Mean} &
\textbf{Sum} & \textbf{Mean} \\
\hline
United States  & 27975547 & 713.55 & 39206 & 11073644 & 282.45 & 477539 & 12.18 \\
Australia      & 21302059 & 889.96 & 23936 & 6776030  & 283.09 & 263585 & 11.01 \\
United Kingdom & 10646196 & 781.66 & 13620 & 4413853  & 324.07 & 157218 & 11.54 \\
Germany        & 8978596  & 809.03 & 11098 & 3359995  & 302.76 & 125720 & 11.33 \\
France         & 8432872  & 766.76 & 10998 & 2880282  & 261.89 & 128995 & 11.73 \\
Canada         & 7935738  & 559.72 & 14178 & 3717296  & 262.19 & 192259 & 13.56 \\
\hline
\end{tabular}
\end{table}


\subsection{Original data quality analysis including visualization}
There are no missing data in our dataset. This is why we did not add any visualization of this parameter. 
    However, there is a timeline gap visible in the figure below. 

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{./figures/3_2_consistency.png}
\caption{Consistency checks in time series and comparison of revenue before and after preprocessing}
\label{fig:consistency}
\end{figure}

Outliers have been identified via IQR method seen in the figure below, but 
they have not been removed. This is because we wanted this data in the dataset 
for better and full analysis. Otherwise the dataset would have lost too much information.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/3_2_boxplots.png}
\caption{IQR applied on some key variables to identify outliers.}
\label{fig:boxplots}
\end{figure}


\subsection{Data preprocessing}
\begin{itemize}[leftmargin=*]
    \item Cleaning steps performed: Duplicates have been dropped.
    \item Missing-value treatment: No missing values were present in the dataset, therefore no treatment was necessary.
    \item Outlier handling: Outliers identified via IQR method (1.5 $\times$ IQR threshold) but intentionally retained. Justification: These extreme values contain valuable business insights (high-value transactions, unusual market events) that would be lost if removed.
    \item Feature engineering: \\Time-based features added: DayOfWeek, DayOfWeek\_Name, WeekOfYear, Quarter, IsWeekend\\ Financial features created: Profit\_Margin, Avg\_Unit\_Profit, High-value flag
    \item Final dataset shape after preprocessing: (8 new features added, no rows removed) \\ Original 113,036 rows $\times$ 18 columns $\rightarrow$ Cleaned 113,036 rows $\times$ 26 columns 
\end{itemize}

\subsection{Preprocessed vs original data visual analysis}
The accuracy of the dataset improved by dropping the duplicates. 
A trade-off would be the possible removal of relevant data (no real duplicate).

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/3_3_comparison.png}
\caption{Comparison of key sales values due to the removal of 1000 duplicates.}
\label{fig:comparison}
\end{figure}

\section{Task 2. Visualization and Exploratory Analysis}

\subsection{Time series visualizations}
\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{./figures/4_1_timeseries.png }
\caption{Time series visualization of sales over time.}
\label{fig:time_series}
\end{figure}

\subsection{Distribution analysis with histograms}
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/4_2_histograms.png}
\caption{Histograms of key numeric variables showing distribution shapes.}
\label{fig:histograms}
\end{figure}

\textbf{Order Quantity:} Nearly symmetric distribution (skewness: 0.378) with light tails, indicating values are concentrated near the center with fewer extreme outliers. The bimodal pattern suggests two distinct purchasing behaviors.

\textbf{Customer Age:} Right-skewed distribution (skewness: 0.524) with normal-like tails. The multimodal pattern reflects age clustering across different customer segments. Mean age of 35.92 years exceeds the median (35.00), confirming right skew with older customers in the tail.

\textbf{Profit Margin:} Left-skewed distribution (skewness: -0.856) with normal-like tails, indicating most transactions cluster at higher profit margins with a tail toward lower margins. The multimodal pattern suggests distinct profit tiers based on product categories.

% \begin{table}[ht]
% \caption{Distribution Characteristics Summary}
% \centering
% \scriptsize
% \setlength{\tabcolsep}{6pt}
% \renewcommand{\arraystretch}{1.0}
% \begin{tabular}{|l|c|c|c|}
% \hline
% \textbf{Variable} & \textbf{Order Quantity} & \textbf{Customer Age} & \textbf{Profit Margin} \\
% \hline
% \textbf{Skewness} & 0.378 (Symmetric) & 0.524 (Right) & -0.856 (Left) \\
% \textbf{Kurtosis} & -1.232 (Light tails) & -0.123 (Normal) & -0.058 (Normal) \\
% \textbf{Modality} & Bimodal & Multimodal & Multimodal \\
% \textbf{Mean} & 11.90 & 35.92 & 47.21 \\
% \textbf{Median} & 10.00 & 35.00 & 52.63 \\
% \textbf{Range} & 1--32 & 17--87 & -3.70--75.00 \\
% \hline
% \end{tabular}
% \end{table}



\subsection{Correlation analysis and heatmaps}
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/4_3_correlation.png}
\caption{Correlation heatmap of key numeric variables.}
\label{fig:correlation}
\end{figure}

Both correlation types have been calculated.
Both types show the same strongest correlations. Those correlations 
are between the financial variables revenue, cost and profit. That makes sense, 
due to higher revenue leading to higher profit. Or higher costs also lead to higher revenue.


\subsection{Monthly pattern analysis}
\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{./figures/4_4_pattern.png}
\caption{Monthly pattern analysis of sales.}
\label{fig:pattern}
\end{figure}

Our dataset doesnt provide any timestamp data for the sales. Therefore, it was grouped into monthly data and reviewed over the whole year. 
The data of every month was then averaged over the years. It is very interesting to see the strong seasonal patterns of bike sales.
Our dataset shows almost identical sales on the weekend compared to the weekdays. This shows that the shop was also open on weekends. 
Which is very unusual for euopean shops. Therefore, it has to be an online shop.
It was interesting to see a deviation in customer age depending on the day of the week.

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{./figures/4_4_pattern_week.png}
\caption{Weekday pattern analysis of sales.}
\label{fig:pattern_week}
\end{figure}


\subsection{Summary of observed patterns, similar to True/False questions}

\begin{enumerate}[leftmargin=*]
    \item \underline{Revenue shows a positive long-term trend over the dataset period} \hspace*{\fill} \textcolor{green}{\textbf{TRUE}}
    \begin{itemize}[leftmargin=2em]
        \item Evidence: 90-day MA grew 308.2\% from start to end
    \end{itemize}
    
    \item \underline{Q4 (Oct-Dec) shows significantly higher sales than other quarters} \hspace*{\fill} \textcolor{red}{\textbf{FALSE}}
    \begin{itemize}[leftmargin=2em]
        \item Evidence: Q4 revenue is not higher than the average of other quarters
    \end{itemize}
    
    \item \underline{Revenue and Profit are strongly positively correlated (r > 0.8)} \hspace*{\fill} \textcolor{green}{\textbf{TRUE}}
    \begin{itemize}[leftmargin=2em]
        \item Evidence: Pearson r = 0.957
    \end{itemize}
    
    \item \underline{Customer age has minimal impact on transaction revenue} \hspace*{\fill} \textcolor{green}{\textbf{TRUE}}
    \begin{itemize}[leftmargin=2em]
        \item Evidence: Age-Revenue correlation r = -0.009
    \end{itemize}
    
    \item \underline{Friday is the best performing day of the week} \hspace*{\fill} \textcolor{red}{\textbf{FALSE}}
    \begin{itemize}[leftmargin=2em]
        \item Evidence: Best day: Wednesday, worst: Thursday (5.2\% difference)
    \end{itemize}
    
    \item \underline{December shows the highest monthly average revenue} \hspace*{\fill} \textcolor{green}{\textbf{TRUE}}
    \begin{itemize}[leftmargin=2em]
        \item Evidence: Best month: Dec, worst: Oct
    \end{itemize}
\end{enumerate}

\section{Task 3. Probability Analysis}

\subsection{Threshold-based probability estimation}
\begin{itemize}[leftmargin=*]
    \item Define threshold(s) and justify choice:
    \item Estimate probabilities of exceeding thresholds:
    \item Visual support (e.g., empirical CDF, bar plot, timeline highlights):
\end{itemize}

\subsection{Cross tabulation analysis}
\begin{itemize}[leftmargin=*]
    \item Define two categorical variables (or binned numeric variables):
    \item Present contingency table and interpret key cells:
\end{itemize}

\subsection{Conditional probability analysis}
\begin{itemize}[leftmargin=*]
    \item Define events $A$ and $B$:
    \item Compute and interpret $P(A)$, $P(B)$, $P(A \mid B)$, $P(B \mid A)$:
    \item Include at least one meaningful comparison and conclusion:
\end{itemize}

\subsection{Summary of observations from each probability task}
\begin{itemize}[leftmargin=*]
    \item Key takeaway from threshold probability:
    \item Key takeaway from crosstab:
    \item Key takeaway from conditional probability:
\end{itemize}

\section{Task 4. Statistical Theory Applications}

\subsection{Law of Large Numbers (LLN) demonstration}
\begin{itemize}[leftmargin=*]
    \item Variable chosen and why it makes sense:
    \item Experiment: show sample mean as $n$ increases:
    \item Plot and short interpretation:
\end{itemize}

\subsection{Central Limit Theorem (CLT) application}
\begin{itemize}[leftmargin=*]
    \item Sampling procedure (sample size, number of trials, with or without replacement):
    \item Show distribution of sample means for increasing $n$:
    \item Plot(s): histogram(s) of sample means and comparison to normal shape:
\end{itemize}

\subsection{Result interpretation}
\begin{itemize}[leftmargin=*]
    \item What LLN showed in your data context:
    \item What CLT showed, and any deviations and why:
\end{itemize}

\section{Task 5. Regression Analysis}

\subsection{Linear or Polynomial model selection}
\begin{itemize}[leftmargin=*]
    \item Define target $y$ and predictors $X$:
    \item Motivation for linear vs polynomial:
    \item Any train-test split rationale (time-aware split if relevant):
\end{itemize}

\subsection{Model fitting and validation}
\begin{itemize}[leftmargin=*]
    \item Fit procedure and preprocessing (scaling, feature selection):
    \item Validation method (holdout, time-series split, etc.):
    \item Metrics reported (RMSE, MAE, $R^2$) and why:
    \item Residual analysis (at least one plot recommended):
\end{itemize}

\subsection{Result interpretation and analysis}
\begin{itemize}[leftmargin=*]
    \item Main effects and practical meaning:
    \item Failure cases or where model performs poorly:
\end{itemize}

\section{Bonus Tasks}
\begin{itemize}[leftmargin=*]
    \item New dataset bonus (10): state why dataset is new and provide link:
    \item Q-Q plot with explanation (5):
    \begin{itemize}
        \item Either for CLT sample means, or regression residuals:
        \item Interpretation of deviations from normality:
    \end{itemize}
    \item Interactive visualizations (up to 10): describe tool used and what interactivity adds:
    \item Cross-validation in regression (5): method used and how results compare to holdout:
    \item Additional exploration (up to 20): clearly state extra tasks and value gained:
\end{itemize}

\section{Key Findings and Conclusions}
\begin{itemize}[leftmargin=*]
    \item Main findings from preprocessing and EDA:
    \item Main findings from probability tasks:
    \item Main findings from LLN and CLT:
    \item Main findings from regression:
    \item Limitations:
    \item What you would do next if you had more time:
\end{itemize}

\section{Reproducibility Notes}
\begin{itemize}[leftmargin=*]
    \item Exact dataset source link and version or download date:
    \item Key libraries used and versions (optional but recommended):
    \item How to run the notebook end-to-end:
\end{itemize}

% Optional
% \section{References}
% \begin{enumerate}
%     \item ...
% \end{enumerate}

\end{document}
