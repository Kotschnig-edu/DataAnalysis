\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{float}
\usepackage{soul}

\geometry{a4paper, margin=1in}

\title{Data Analysis Final Assignment Report}
\author{% 
  Team: Analog Avengers\\ 
  Eingang Fabian
  \And
  Kotschnig Thomas
  \And 
  Krenn Matthias 
  }

\date{}

\begin{document}
\maketitle

\section{Contributions}
\begin{itemize}[leftmargin=*]
    \item Eingang Fabian: Dataset selection and acquisition, Data quality analysis and preprocessing pipeline
    \item Kotschnig Thomas: Visualizations and EDA, Probability analysis tasks
    \item Krenn Matthias: Regression modeling and interpretation, Report writing and figure polishing

\end{itemize}

\section{Dataset Description}
\begin{itemize}[leftmargin=*]
    \item "Bike sales in Europe" from https://www.kaggle.com/datasets/sadiqshah/bike-sales-in-europe
    \item It has more than 100k entries of sales data from different countries. Streching from 2011 to 2016, with a daily sampling frequency.
    \item Key variables analyzed: customer age, order quantity, unit cost, unit price, profit, cost, revenue
    \item Shape: 113036 rows x 18 columns
    \item No missing data, however, the entry of some dates is missing completely. 
    This resolves in no missing data, but inconsistent time series. 
    There is only one bigger gap, therefore we decided for it to be okay.
\end{itemize}

\section{Task 1. Data Preprocessing and Basic Analysis}

\subsection{Basic statistical analysis using pandas}
Statistical summary of key numeric variables was obtained using pandas \texttt{describe()} function:
\begin{table}[h!]
\caption{Desriptive statistics}
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{|l|r|r|r|r|r|r|r|}
\hline
 & \textbf{Customer Age} & \textbf{Order Qty} & \textbf{Unit Cost} & \textbf{Unit Price} & \textbf{Profit} & \textbf{Cost} & \textbf{Revenue} \\
\hline
Count & 113036 & 113036 & 113036 & 113036 & 113036 & 113036 & 113036 \\
Mean  & 35.92  & 11.90  & 267.30 & 452.94 & 285.05 & 469.32 & 754.37 \\
Std   & 11.02  & 9.56   & 549.84 & 922.07 & 453.89 & 884.87 & 1309.09 \\
Min   & 17     & 1      & 1      & 2      & -30    & 1      & 2 \\
25\%  & 28     & 2      & 2      & 5      & 29     & 28     & 63 \\
50\%  & 35     & 10     & 9      & 24     & 101    & 108    & 223 \\
75\%  & 43     & 20     & 42     & 70     & 358    & 432    & 800 \\
Max   & 87     & 32     & 2171   & 3578   & 15096  & 42978  & 58074 \\
\hline
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Grouped summary of Revenue, Profit, and Order Quantity by Country}
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{|l|ccc|cc|cc|}
\hline
\textbf{Country} &
\multicolumn{3}{c|}{\textbf{Revenue}} &
\multicolumn{2}{c|}{\textbf{Profit}} &
\multicolumn{2}{c|}{\textbf{Order Quantity}} \\
\hline
 &
\textbf{Sum} & \textbf{Mean} & \textbf{Count} &
\textbf{Sum} & \textbf{Mean} &
\textbf{Sum} & \textbf{Mean} \\
\hline
United States  & 27975547 & 713.55 & 39206 & 11073644 & 282.45 & 477539 & 12.18 \\
Australia      & 21302059 & 889.96 & 23936 & 6776030  & 283.09 & 263585 & 11.01 \\
United Kingdom & 10646196 & 781.66 & 13620 & 4413853  & 324.07 & 157218 & 11.54 \\
Germany        & 8978596  & 809.03 & 11098 & 3359995  & 302.76 & 125720 & 11.33 \\
France         & 8432872  & 766.76 & 10998 & 2880282  & 261.89 & 128995 & 11.73 \\
Canada         & 7935738  & 559.72 & 14178 & 3717296  & 262.19 & 192259 & 13.56 \\
\hline
\end{tabular}
\end{table}


\subsection{Original data quality analysis including visualization}
There are no missing data in our dataset. This is why we did not add any visualization of this parameter. 
    However, there is a timeline gap visible in the figure below. 

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{./figures/3_2_consistency.png}
\caption{Consistency checks in time series and comparison of revenue before and after preprocessing}
\label{fig:consistency}
\end{figure}

Outliers have been identified via IQR method seen in the figure below, but 
they have not been removed. This is because we wanted this data in the dataset 
for better and full analysis. Otherwise the dataset would have lost too much information.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/3_2_boxplots.png}
\caption{IQR applied on some key variables to identify outliers.}
\label{fig:boxplots}
\end{figure}


\subsection{Data preprocessing}
\begin{itemize}[leftmargin=*]
    \item Cleaning steps performed: Duplicates have been dropped.
    \item Missing-value treatment: No missing values were present in the dataset, therefore no treatment was necessary.
    \item Outlier handling: Outliers identified via IQR method (1.5 $\times$ IQR threshold) but intentionally retained. Justification: These extreme values contain valuable business insights (high-value transactions, unusual market events) that would be lost if removed.
    \item Feature engineering: \\Time-based features added: DayOfWeek, DayOfWeek\_Name, WeekOfYear, Quarter, IsWeekend\\ Financial features created: Profit\_Margin, Avg\_Unit\_Profit, High-value flag
    \item Final dataset shape after preprocessing: (8 new features added, no rows removed) \\ Original 113,036 rows $\times$ 18 columns $\rightarrow$ Cleaned 113,036 rows $\times$ 26 columns 
\end{itemize}

\subsection{Preprocessed vs original data visual analysis}
The accuracy of the dataset improved by dropping the duplicates. 
A trade-off would be the possible removal of relevant data (no real duplicate).

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/3_3_comparison.png}
\caption{Comparison of key sales values due to the removal of 1000 duplicates.}
\label{fig:comparison}
\end{figure}








\section{Task 2. Visualization and Exploratory Analysis}

\subsection{Time series visualizations}
\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{./figures/4_1_timeseries.png }
\caption{Time series visualization of sales over time.}
\label{fig:time_series}
\end{figure}

\subsection{Distribution analysis with histograms}
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/4_2_histograms.png}
\caption{Histograms of key numeric variables showing distribution shapes.}
\label{fig:histograms}
\end{figure}

\textbf{Order Quantity:} Nearly symmetric distribution (skewness: 0.378) with light tails, indicating values are concentrated near the center with fewer extreme outliers. The bimodal pattern suggests two distinct purchasing behaviors.

\textbf{Customer Age:} Right-skewed distribution (skewness: 0.524) with normal-like tails. The multimodal pattern reflects age clustering across different customer segments. Mean age of 35.92 years exceeds the median (35.00), confirming right skew with older customers in the tail.

\textbf{Profit Margin:} Left-skewed distribution (skewness: -0.856) with normal-like tails, indicating most transactions cluster at higher profit margins with a tail toward lower margins. The multimodal pattern suggests distinct profit tiers based on product categories.


\subsection{Correlation analysis and heatmaps}
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/4_3_correlation.png}
\caption{Correlation heatmap of key numeric variables.}
\label{fig:correlation}
\end{figure}

Both correlation types have been calculated.
Both types show the same strongest correlations. Those correlations 
are between the financial variables revenue, cost and profit. That makes sense, 
due to higher revenue leading to higher profit. Or higher costs also lead to higher revenue.


\subsection{Monthly pattern analysis}
\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{./figures/4_4_pattern.png}
\caption{Monthly pattern analysis of sales.}
\label{fig:pattern}
\end{figure}

Our dataset doesnt provide any timestamp data for the sales. Therefore, it was grouped into monthly data and reviewed over the whole year. 
The data of every month was then averaged over the years. It is very interesting to see the strong seasonal patterns of bike sales.
Our dataset shows almost identical sales on the weekend compared to the weekdays. This shows that the shop was also open on weekends. 
Which is very unusual for euopean shops. Therefore, it has to be an online shop.
It was interesting to see a deviation in customer age depending on the day of the week.

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{./figures/4_4_pattern_week.png}
\caption{Weekday pattern analysis of sales.}
\label{fig:pattern_week}
\end{figure}


\subsection{Summary of observed patterns, similar to True/False questions}

\begin{enumerate}[leftmargin=*]
    \item \underline{Revenue shows a positive long-term trend over the dataset period} \hspace*{\fill} \textcolor{green}{\textbf{TRUE}}
    \begin{itemize}[leftmargin=2em]
        \item Evidence: 90-day MA grew 308.2\% from start to end
    \end{itemize}
    
    \item \underline{Q4 (Oct-Dec) shows significantly higher sales than other quarters} \hspace*{\fill} \textcolor{red}{\textbf{FALSE}}
    \begin{itemize}[leftmargin=2em]
        \item Evidence: Q4 revenue is not higher than the average of other quarters
    \end{itemize}
    
    \item \underline{Revenue and Profit are strongly positively correlated (r \textgreater 0.8)} \hspace*{\fill} \textcolor{green}{\textbf{TRUE}}
    \begin{itemize}[leftmargin=2em]
        \item Evidence: Pearson r = 0.957
    \end{itemize}
    
    \item \underline{Customer age has minimal impact on transaction revenue} \hspace*{\fill} \textcolor{green}{\textbf{TRUE}}
    \begin{itemize}[leftmargin=2em]
        \item Evidence: Age-Revenue correlation r = -0.009
    \end{itemize}
    
    \item \underline{December shows the highest monthly average revenue} \hspace*{\fill} \textcolor{green}{\textbf{TRUE}}
    \begin{itemize}[leftmargin=2em]
        \item Evidence: Best month: Dec, worst: Oct
    \end{itemize}
\end{enumerate}








\section{Task 3. Probability Analysis}

\subsection{Threshold-based probability estimation}

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{./figures/5_1_threshold_prob.png}
\caption{Threshold-based probability estimation visualization.}
\label{fig:threshold_prob}
\end{figure}
The three thresholds chosen are:
\begin{itemize}[leftmargin=*]
    \item High value transaction $\rightarrow$ 75\% was used as threshold
    \item Order quantity $\rightarrow$ everything above 10 units was considered high
    \item High profit transaction $\rightarrow$ mean profit value was used as threshold
\end{itemize}

\subsection{Cross tabulation analysis}
\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{./figures/5_2_cross_tab.png}
\caption{Cross tabulation analysis visualization.}
\label{fig:cross_tab}
\end{figure}
It is interesting to see that the product category is not not influenced by the 
customer age. However, in the right cross tab it is visible that in canada the 
amount of bike sales is much lower than in the rest of the countries. But the 
accessories are sold way more often in canada than in the other countries.








\newpage
\subsection{Conditional probability analysis}
Question: Which age group is most likely to place large orders?\\

\begin{itemize}[leftmargin=*]
    \item Events: $A$ = Customer age group, $B$ = Order quantity  
    \item \hl{Compute and interpret $P(A)$, $P(B)$, $P(A \mid B)$, $P(B \mid A)$:}
    \item \hl{Include at least one meaningful comparison and conclusion:}
\end{itemize}
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{./figures/5_3_conditional_prob.png}
\caption{Conditional probability analysis visualization.}
\label{fig:conditional_prob}
\end{figure}
The conclusion of this analysis is that customers above 45 years are more likely
to place large orders (more than 10 units).

\subsection{Summary of observations from each probability task}
\begin{itemize}[leftmargin=*]
    \item Key takeaway from threshold probability: Choosing thresholds requires 
    care‚Äîtoo high or low can misrepresent risk; advantage is simplicity and 
    clear segmentation, but it ignores interactions beyond the set cutoffs.
    \item Key takeaway from crosstab: Useful for detecting dependencies between 
    categorical variables, but small sample sizes can give misleading 
    p-values; it‚Äôs easy to interpret but limited to pairwise relationships.
    \item Key takeaway from conditional probability: Shows how one event affects 
    another, highlighting trends; risk lies in misinterpreting correlation as 
    causation, though it provides actionable insights for targeting specific segments.
\end{itemize}
\hl{check this again}

\newpage
\section{Task 4. Statistical Theory Applications}

\subsection{Law of Large Numbers (LLN) demonstration}
For the analysis the revenue was chosen, because it make a lot of sense for 
financial analysis. Knowing the mean revenue of 100 sales for example and how 
fast is converges to the sample mean $n$. 
It is visible that the sample mean converges to the population mean with 
increasing sample size.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/6_1_LLN.png}
\caption{LLN demonstration visualization.}
\label{fig:LLN}
\end{figure}


\subsection{Central Limit Theorem (CLT) application}
We used different sample sizes to show the CLT. Ranging from 10 to 500.
For every sample size 2000 number of trials were performed with replacement.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/6_2_CLT.png}
\caption{CLT demonstration visualization.}
\label{fig:CLT}
\end{figure}

\hl{l√∂schen?}
\subsection{Result interpretation}
\begin{itemize}[leftmargin=*]
    \item What LLN showed in your data context:\\
That the sample mean of the revenue can be used as an mean value 
for the average revenue per sale.
    \item What CLT showed, and any deviations and why:\\
The Central Limit Theorem is demonstrated by the sampling 
distributions of the sample mean becoming increasingly normal 
as sample size increases. For smaller sample sizes, slight 
skewness is observed due to the underlying revenue distribution,
 but this deviation decreases for larger samples. The standard 
 error of the sample mean decreases at the theoretical 
 rate \( \sigma / \sqrt{n} \), closely matching empirical estimates.
\end{itemize}









\newpage
\section{Task 5. Regression Analysis}

\subsection{Linear or Polynomial model selection}
\begin{itemize}[leftmargin=*]
    \item Define target $y$ and predictors $X$:\\
    The target variable 
ùë¶
y is daily revenue, while the predictors 
ùëã
X are explanatory variables capturing temporal, operational, and demand-related effects.
    \item Compared Linear Regression vs Polynomial Regression (degree 2, captures non-linearity).
    Selection based on test R¬≤ and overfitting gap. Simpler model preferred when performance is similar.
    \item Time-based 80/20 split respects temporal ordering. StandardScaler 
    applied for stable coefficient estimation. Both models evaluated on 
    unseen future data.
\end{itemize}

\subsection{Model fitting and validation}
\begin{itemize}[leftmargin=*]
    \item Fit procedure and preprocessing (scaling, feature selection):
    \item Validation method (holdout, time-series split, etc.):
    \item Metrics reported (RMSE, MAE, $R^2$) and why:
    \item Residual analysis (at least one plot recommended):
\end{itemize}

\subsection{Result interpretation and analysis}
\begin{itemize}[leftmargin=*]
    \item Main effects and practical meaning:
    \item Failure cases or where model performs poorly:
\end{itemize}

\section{Bonus Tasks}
\begin{itemize}[leftmargin=*]
    \item New dataset bonus (10): state why dataset is new and provide link:
    \item Q-Q plot with explanation (5):
    \begin{itemize}
        \item Either for CLT sample means, or regression residuals:
        \item Interpretation of deviations from normality:
    \end{itemize}
    \item Interactive visualizations (up to 10): describe tool used and what interactivity adds:
    \item Cross-validation in regression (5): method used and how results compare to holdout:
    \item Additional exploration (up to 20): clearly state extra tasks and value gained:
\end{itemize}

\section{Key Findings and Conclusions}
\begin{itemize}[leftmargin=*]
    \item Main findings from preprocessing and EDA:
    \item Main findings from probability tasks:
    \item Main findings from LLN and CLT:
    \item Main findings from regression:
    \item Limitations:
    \item What you would do next if you had more time:
\end{itemize}

\section{Reproducibility Notes}
\begin{itemize}[leftmargin=*]
    \item Exact dataset source link and version or download date:
    \item Key libraries used and versions (optional but recommended):
    \item How to run the notebook end-to-end:
\end{itemize}

% Optional
% \section{References}
% \begin{enumerate}
%     \item ...
% \end{enumerate}

\end{document}
